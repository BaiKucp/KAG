#------------project configuration start----------------#
openie_llm: &openie_llm
  type: openai
  base_url:  https://api.siliconflow.cn/v1
  api_key: sk-xmubwjeopdksjenuqmsjkvvipldehyacnmkvghpyoekwqdzz
  model: Qwen/Qwen3-8B
  enable_check: false

chat_llm: &chat_llm
  type: openai
  base_url:  https://api.siliconflow.cn/v1
  api_key: sk-xmubwjeopdksjenuqmsjkvvipldehyacnmkvghpyoekwqdzz
  model: deepseek-ai/DeepSeek-V3.2
  stream: true
  enable_check: false


vectorize_model: &vectorize_model
  api_key: sk-xmubwjeopdksjenuqmsjkvvipldehyacnmkvghpyoekwqdzz
  base_url:  https://api.siliconflow.cn/v1
  model: Qwen/Qwen3-Embedding-8B
  type: openai
  vector_dimensions: 4096
vectorizer: *vectorize_model

log:
  level: INFO

project:
  biz_scene: default
  host_addr: http://127.0.0.1:8887
  id: '60'
  language: zh
  namespace: Medicine
#------------project configuration end----------------#

#------------kag-builder configuration start----------------#
kag_builder_pipeline:
  chain:
    type: unstructured_builder_chain
    extractor:
      type: medical_knowledge_unit_extractor # Use custom extractor for Medicine project
      llm: *openie_llm
      # Catalog file will be passed dynamically or defaulted here. 
      # Since we want to support multi-book with different catalogs, we might rely on indexer.py to pass specific config,
      # OR set a default catalog here. For now, we set a default relative path, but note that pure YAML config limits dynamic catalog switching unless we strictly use indexer.py to override it.
      catalog_file: ./builder/data/14.《病理学（第10版）》-绪论到第四章.md # Default catalog if not overridden
      vectorize_model_config: *vectorize_model
    reader:
      type: mix_reader # Supports md, pdf, docx, txt
    post_processor:
      type: kag_post_processor
    splitter:
      type: length_splitter
      split_length: 500
      window_length: 100
    vectorizer:
      type: batch_vectorizer
      vectorize_model: *vectorize_model
    writer:
      type: kg_writer
  num_threads_per_chain: 6
  num_chains: 16
  scanner:
    type: dir_file_scanner
    file_suffix: [md, pdf, docx, txt]
#------------kag-builder configuration end----------------#

#------------kag-solver configuration start----------------#
search_api: &search_api
  type: openspg_search_api #kag.solver.tools.search_api.impl.openspg_search_api.OpenSPGSearchAPI

graph_api: &graph_api
  type: openspg_graph_api #kag.solver.tools.graph_api.impl.openspg_graph_api.OpenSPGGraphApi


kg_cs: &kg_cs
  type: kg_cs_open_spg
  priority: 0
  path_select:
    type: exact_one_hop_select
    graph_api: *graph_api
    search_api: *search_api
  entity_linking:
    type: entity_linking
    graph_api: *graph_api
    search_api: *search_api
    recognition_threshold: 0.5
    exclude_types:
      - Chunk
      - AtomicQuery
      - KnowledgeUnit
      - Summary
      - Outline
      - Doc

kg_fr: &kg_fr
  type: kg_fr_knowledge_unit
  top_k: 12
  graph_api: *graph_api
  search_api: *search_api
  vectorize_model: *vectorize_model
  path_select:
    type: fuzzy_one_hop_select
    llm_client: *openie_llm
    graph_api: *graph_api
    search_api: *search_api
  ppr_chunk_retriever_tool:
    type: ppr_chunk_retriever
    llm_client: *chat_llm
    graph_api: *graph_api
    search_api: *search_api
    recall_num: 50
  entity_linking:
    type: entity_linking
    graph_api: *graph_api
    search_api: *search_api
    recognition_threshold: 0.5
    exclude_types:
      - Chunk
      - AtomicQuery
      - KnowledgeUnit
      - Summary
      - Outline
      - Doc

rc: &rc
  type: rc_open_spg
  vector_chunk_retriever:
    type: vector_chunk_retriever
    vectorize_model: *vectorize_model
    score_threshold: 0.65
    search_api: *search_api
    top_k: 24
  graph_api: *graph_api
  search_api: *search_api
  vectorize_model: *vectorize_model
  top_k: 12

kag_hybrid_executor: &kag_hybrid_executor_conf
  type: kag_hybrid_retrieval_executor
  retrievers:
    - *kg_cs
    - *kg_fr
    - *rc
  merger:
    type: kag_merger
  enable_summary: false

kag_output_executor: &kag_output_executor_conf
  type: kag_output_executor
  llm_module: *chat_llm

kag_deduce_executor: &kag_deduce_executor_conf
  type: kag_deduce_executor
  llm_module: *chat_llm

py_code_based_math_executor: &py_code_based_math_executor_conf
  type: py_code_based_math_executor
  llm: *chat_llm

kag_solver_pipeline:
  type: kag_static_pipeline
  planner:
    type: lf_kag_static_planner
    llm: *chat_llm
    plan_prompt:
      type: default_lf_static_planning
    rewrite_prompt:
      type: default_rewrite_sub_task_query
  executors:
    - *kag_hybrid_executor_conf
    - *kag_output_executor_conf
  generator:
    type: llm_generator
    llm_client: *chat_llm
    generated_prompt:
      type: medical_refer_generator_prompt
    chunk_reranker:
      type: rerank_by_vector
      vectorize_model: *vectorize_model
      rerank_top_k: 12
    enable_ref: true

#------------kag-solver configuration end----------------#
